{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b59ee0a-3324-4131-bfd2-ba43129704e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUR: ALS (matrix-free) vs EM (dense) ===\n",
      "\n",
      "--- ALS SUR ---\n",
      "mode: SUR\n",
      "solver: ALS\n",
      "K: 120\n",
      "p: 3\n",
      "k: 8\n",
      "N_tr: 240\n",
      "N_te: 120\n",
      "lam_F: 0.001\n",
      "lam_B: 0.001\n",
      "alpha: 1.06\n",
      "gamma: 3.1151434705040384e-05\n",
      "tau: 3.5380366157200536\n",
      "s: 1.03515625\n",
      "Sec_fit: 0.10996603965759277\n",
      "Mem_MB_est: 0.00864\n",
      "Test_MSE: 11.541302021105702\n",
      "Test_NLL_perN_cal: -48.965705275738294\n",
      "\n",
      "[train (pre-calib raw F,D)]  K=120  N=240\n",
      "  z cov@90%: overall=0.999, min_eq=0.992, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=119.98 (≈K), var=201.35 (≈2K)\n",
      "  frac m2>χ2_95=0.033 (≈0.05), >χ2_99=0.004 (≈0.01)\n",
      "  whiten frob_dev=0.813, offdiag_max=0.564\n",
      "\n",
      "[test  (calibrated)]  K=120  N=120\n",
      "  z cov@90%: overall=0.999, min_eq=0.983, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=0.992, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=129.60 (≈K), var=317.14 (≈2K)\n",
      "  frac m2>χ2_95=0.150 (≈0.05), >χ2_99=0.050 (≈0.01)\n",
      "  whiten frob_dev=12.148, offdiag_max=7.715\n",
      "\n",
      "--- EM  SUR ---\n",
      "mode: SUR\n",
      "solver: EM\n",
      "K: 120\n",
      "p: 3\n",
      "k: 8\n",
      "N_tr: 240\n",
      "N_te: 120\n",
      "lam_F: 0.001\n",
      "lam_B: 0.001\n",
      "alpha: 1.02\n",
      "gamma: 2.6337358638047247e-05\n",
      "tau: 3.1228764698501026\n",
      "s: 1.0656249999999998\n",
      "Sec_fit: 3.6734120845794678\n",
      "Mem_MB_est: 1.152\n",
      "Test_MSE: 11.540957609110604\n",
      "Test_NLL_perN_cal: -48.63539333079413\n",
      "\n",
      "[train (pre-calib raw F,D)]  K=120  N=240\n",
      "  z cov@90%: overall=0.998, min_eq=0.992, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=0.996, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=122.10 (≈K), var=205.11 (≈2K)\n",
      "  frac m2>χ2_95=0.042 (≈0.05), >χ2_99=0.004 (≈0.01)\n",
      "  whiten frob_dev=0.882, offdiag_max=0.657\n",
      "\n",
      "[test  (calibrated)]  K=120  N=120\n",
      "  z cov@90%: overall=0.998, min_eq=0.983, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=0.992, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=132.30 (≈K), var=330.66 (≈2K)\n",
      "  frac m2>χ2_95=0.192 (≈0.05), >χ2_99=0.067 (≈0.01)\n",
      "  whiten frob_dev=13.294, offdiag_max=11.330\n",
      "\n",
      "--- SUR: Deltas (ALS - EM) ---\n",
      "Test_MSE: +0.0003\n",
      "Test_NLL_perN_cal: -0.3303\n",
      "Sec_fit: -3.5634\n",
      "Mem_MB_est: -1.1434\n",
      "=== GLS: ALS (matrix-free) vs EM (dense) ===\n",
      "\n",
      "--- ALS GLS ---\n",
      "mode: GLS\n",
      "solver: ALS\n",
      "K: 120\n",
      "k: 8\n",
      "p_list_summary: (2, 4, 8)\n",
      "N_tr: 240\n",
      "N_te: 120\n",
      "lam_F: 0.001\n",
      "lam_B: 0.001\n",
      "alpha: 0.8400000000000001\n",
      "gamma: 3.252448479024335e-05\n",
      "tau: 4.3336740809715435\n",
      "s: 1.390625\n",
      "Sec_fit: 0.10497522354125977\n",
      "Mem_MB_est: 0.00864\n",
      "Test_MSE: 11.897996813896631\n",
      "Test_NLL_perN_cal: -50.19048962619187\n",
      "\n",
      "[train (pre-calib raw F,D)]  K=120  N=240\n",
      "  z cov@90%: overall=0.999, min_eq=0.992, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=0.996, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=119.72 (≈K), var=266.07 (≈2K)\n",
      "  frac m2>χ2_95=0.062 (≈0.05), >χ2_99=0.008 (≈0.01)\n",
      "  whiten frob_dev=0.815, offdiag_max=0.425\n",
      "\n",
      "[test  (calibrated)]  K=120  N=120\n",
      "  z cov@90%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=126.78 (≈K), var=292.78 (≈2K)\n",
      "  frac m2>χ2_95=0.108 (≈0.05), >χ2_99=0.042 (≈0.01)\n",
      "  whiten frob_dev=11.514, offdiag_max=5.949\n",
      "\n",
      "--- EM  GLS ---\n",
      "mode: GLS\n",
      "solver: EM\n",
      "K: 120\n",
      "k: 8\n",
      "p_list_summary: (2, 4, 8)\n",
      "N_tr: 240\n",
      "N_te: 120\n",
      "lam_F: 0.001\n",
      "lam_B: 0.001\n",
      "alpha: 1.04\n",
      "gamma: 1.5544933637700038e-05\n",
      "tau: 2.9612907244726427\n",
      "s: 1.10625\n",
      "Sec_fit: 1.7943191528320312\n",
      "Mem_MB_est: 2.552832\n",
      "Test_MSE: 11.89802358579086\n",
      "Test_NLL_perN_cal: -51.30018730360786\n",
      "\n",
      "[train (pre-calib raw F,D)]  K=120  N=240\n",
      "  z cov@90%: overall=0.998, min_eq=0.983, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=0.992, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=122.19 (≈K), var=269.99 (≈2K)\n",
      "  frac m2>χ2_95=0.087 (≈0.05), >χ2_99=0.021 (≈0.01)\n",
      "  whiten frob_dev=0.904, offdiag_max=0.631\n",
      "\n",
      "[test  (calibrated)]  K=120  N=120\n",
      "  z cov@90%: overall=0.999, min_eq=0.992, max_eq=1.000\n",
      "  z cov@95%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  z cov@99%: overall=1.000, min_eq=1.000, max_eq=1.000\n",
      "  m2 mean=125.08 (≈K), var=285.79 (≈2K)\n",
      "  frac m2>χ2_95=0.092 (≈0.05), >χ2_99=0.033 (≈0.01)\n",
      "  whiten frob_dev=12.628, offdiag_max=9.824\n",
      "\n",
      "--- GLS: Deltas (ALS - EM) ---\n",
      "Test_MSE: -0.0000\n",
      "Test_NLL_perN_cal: +1.1097\n",
      "Sec_fit: -1.6893\n",
      "Mem_MB_est: -2.5442\n"
     ]
    }
   ],
   "source": [
    "# experiments/run_comparison.py\n",
    "# Compare ALS (matrix-free) vs EM (dense) for low-rank+diag GLS/SUR.\n",
    "# Assumes `lowrank_gls` is importable (i.e., package root on PYTHONPATH).\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from lowrank_gls import (\n",
    "    # solvers\n",
    "    als_gls,\n",
    "    # calibration\n",
    "    calibrate_alpha_gamma_s_cv3_conservative,\n",
    "    # diagnostics & metrics\n",
    "    coverage_and_mahalanobis,\n",
    "    em_gls,\n",
    "    finalize_on_dataset,\n",
    "    mse,\n",
    "    predict_Y,\n",
    "    test_nll,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Simulation (SUR and GLS)\n",
    "# =========================\n",
    "\n",
    "def simulate_sur(N_tr, N_te, K, p, k, seed=0):\n",
    "    \"\"\"SUR: X_j share a base + idio noise; latent k factors + idio diag noise.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = N_tr + N_te\n",
    "    base = rng.standard_normal((N, p))\n",
    "    Xs = [base + 0.50*rng.standard_normal((N, p)) for _ in range(K)]\n",
    "    B  = [rng.standard_normal((p, 1)) for _ in range(K)]\n",
    "    F0 = 1.2 * rng.standard_normal((K, k))\n",
    "    D0 = 0.02 + 0.15 * rng.random(K)\n",
    "    U  = rng.standard_normal((N, k))\n",
    "    Y  = predict_Y(Xs, B) + U @ F0.T + rng.standard_normal((N, K)) * np.sqrt(D0)[None, :]\n",
    "    # split\n",
    "    Xs_tr = [X[:N_tr] for X in Xs]\n",
    "    Xs_te = [X[N_tr:] for X in Xs]\n",
    "    Y_tr  = Y[:N_tr]\n",
    "    Y_te  = Y[N_tr:]\n",
    "    return Xs_tr, Y_tr, Xs_te, Y_te\n",
    "\n",
    "def simulate_gls(N_tr, N_te, p_list, k, seed=0):\n",
    "    \"\"\"General GLS: per-equation p_j; latent k factors + idio diag noise.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    K = len(p_list)\n",
    "    N = N_tr + N_te\n",
    "    # give all equations a shared base in their own feature space + noise\n",
    "    Xs = []\n",
    "    for p in p_list:\n",
    "        base = rng.standard_normal((N, p))\n",
    "        Xs.append(base + 0.50*rng.standard_normal((N, p)))\n",
    "    B  = [rng.standard_normal((p, 1)) for p in p_list]\n",
    "    F0 = 1.2 * rng.standard_normal((K, k))\n",
    "    D0 = 0.02 + 0.15 * rng.random(K)\n",
    "    U  = rng.standard_normal((N, k))\n",
    "    Y  = predict_Y(Xs, B) + U @ F0.T + rng.standard_normal((N, K)) * np.sqrt(D0)[None, :]\n",
    "    # split\n",
    "    Xs_tr = [X[:N_tr] for X in Xs]\n",
    "    Xs_te = [X[N_tr:] for X in Xs]\n",
    "    Y_tr  = Y[:N_tr]\n",
    "    Y_te  = Y[N_tr:]\n",
    "    return Xs_tr, Y_tr, Xs_te, Y_te\n",
    "\n",
    "# =========================\n",
    "# Pretty printing\n",
    "# =========================\n",
    "\n",
    "def print_metrics_block(title, metrics, diag_train, diag_test):\n",
    "    print(title)\n",
    "    for k in [\"mode\",\"solver\",\"K\",\"p\",\"k\",\"p_list_summary\",\"N_tr\",\"N_te\",\n",
    "              \"lam_F\",\"lam_B\",\"alpha\",\"gamma\",\"tau\",\"s\",\"Sec_fit\",\"Mem_MB_est\",\n",
    "              \"Test_MSE\",\"Test_NLL_perN_cal\"]:\n",
    "        if k in metrics and metrics[k] is not None:\n",
    "            print(f\"{k}: {metrics[k]}\")\n",
    "    if diag_train is not None:\n",
    "        print(\"\\n[train (pre-calib raw F,D)]  \"\n",
    "              f\"K={diag_train['K']}  N={diag_train['N']}\")\n",
    "        _pp_diag(diag_train)\n",
    "    if diag_test is not None:\n",
    "        print(\"\\n[test  (calibrated)]  \"\n",
    "              f\"K={diag_test['K']}  N={diag_test['N']}\")\n",
    "        _pp_diag(diag_test)\n",
    "\n",
    "def _pp_diag(d):\n",
    "    for k in [\"cov@90%\",\"cov@95%\",\"cov@99%\"]:\n",
    "        if k in d[\"z_coverage_overall\"]:\n",
    "            v = d[\"z_coverage_overall\"][k]\n",
    "            print(f\"  z {k}: overall={v['overall']:.3f}, \"\n",
    "                  f\"min_eq={v['min_eq']:.3f}, max_eq={v['max_eq']:.3f}\")\n",
    "    print(f\"  m2 mean={d['m2_mean']:.2f} (≈K), var={d['m2_var']:.2f} (≈2K)\")\n",
    "    print(f\"  frac m2>χ2_95={d['m2_frac_gt_95']:.3f} (≈0.05), \"\n",
    "          f\">χ2_99={d['m2_frac_gt_99']:.3f} (≈0.01)\")\n",
    "    print(f\"  whiten frob_dev={d['whiten_frob_dev']:.3f}, \"\n",
    "          f\"offdiag_max={d['whiten_offdiag_max']:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# Pipelines (ALS / EM)\n",
    "# =========================\n",
    "\n",
    "def run_sur_pipeline_als_cv31(*,\n",
    "    N_tr, N_te, K, p, k, seed,\n",
    "    lam_F=1e-3, lam_B=1e-3, sweeps=12,\n",
    "    alpha_grid=np.linspace(0.78, 1.10, 17), Kfolds=3, val_frac=0.5,\n",
    "    tail_target=0.03, cov_target=0.975, r_boost=2, d_floor=1e-6,\n",
    "    use_cg_beta=True, cg_maxit=800, cg_tol=3e-7, use_diag_precond=True\n",
    "):\n",
    "    Xs_tr, Y_tr, Xs_te, Y_te = simulate_sur(N_tr, N_te, K, p, k, seed=seed)\n",
    "\n",
    "    t0 = time.time()\n",
    "    B, F, D, mem_mb, _ = als_gls(\n",
    "        Xs_tr, Y_tr, k,\n",
    "        lam_F=lam_F, lam_B=lam_B, sweeps=sweeps, d_floor=d_floor,\n",
    "        use_cg_beta=use_cg_beta, cg_maxit=cg_maxit, cg_tol=cg_tol,\n",
    "        use_diag_precond=use_diag_precond\n",
    "    )\n",
    "    sec_fit = time.time() - t0\n",
    "\n",
    "    # pre-calibration diagnostics on train\n",
    "    diag_tr = coverage_and_mahalanobis(Y_tr, Xs_tr, B, F, D)\n",
    "\n",
    "    # CV calibration on train, finalize on test\n",
    "    alpha, gamma, tau, s, use_eig = calibrate_alpha_gamma_s_cv3_conservative(\n",
    "        Xs_tr, Y_tr, B, F, D,\n",
    "        alpha_grid=alpha_grid, Kfolds=Kfolds, val_frac=val_frac,\n",
    "        tail_target=tail_target, cov_target=cov_target,\n",
    "        r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "    F_fin, D_fin = finalize_on_dataset(\n",
    "        Xs_te, Y_te, B, F, D, alpha, gamma, tau, s,\n",
    "        use_eig=use_eig, r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "\n",
    "    # metrics\n",
    "    Yhat_te = predict_Y(Xs_te, B)\n",
    "    test_mse = mse(Y_te, Yhat_te)\n",
    "    test_nll_cal = test_nll(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "    diag_te = coverage_and_mahalanobis(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "\n",
    "    metrics = {\n",
    "        \"mode\": \"SUR\", \"solver\": \"ALS\",\n",
    "        \"K\": K, \"p\": p, \"k\": k,\n",
    "        \"N_tr\": N_tr, \"N_te\": N_te,\n",
    "        \"lam_F\": lam_F, \"lam_B\": lam_B,\n",
    "        \"alpha\": alpha, \"gamma\": gamma, \"tau\": tau, \"s\": s,\n",
    "        \"Sec_fit\": sec_fit, \"Mem_MB_est\": mem_mb,\n",
    "        \"Test_MSE\": test_mse, \"Test_NLL_perN_cal\": test_nll_cal,\n",
    "    }\n",
    "    return metrics, (B, F, D), (F_fin, D_fin), diag_tr, diag_te\n",
    "\n",
    "def run_sur_pipeline_em_cv31(*,\n",
    "    N_tr, N_te, K, p, k, seed,\n",
    "    lam_F=1e-3, lam_B=1e-3, iters=45,\n",
    "    alpha_grid=np.linspace(0.78, 1.10, 17), Kfolds=3, val_frac=0.5,\n",
    "    tail_target=0.03, cov_target=0.975, r_boost=2, d_floor=1e-6\n",
    "):\n",
    "    Xs_tr, Y_tr, Xs_te, Y_te = simulate_sur(N_tr, N_te, K, p, k, seed=seed)\n",
    "\n",
    "    t0 = time.time()\n",
    "    B, F, D, mem_mb, _ = em_gls(\n",
    "        Xs_tr, Y_tr, k, lam_F=lam_F, lam_B=lam_B,\n",
    "        iters=iters, d_floor=d_floor\n",
    "    )\n",
    "    sec_fit = time.time() - t0\n",
    "\n",
    "    diag_tr = coverage_and_mahalanobis(Y_tr, Xs_tr, B, F, D)\n",
    "\n",
    "    alpha, gamma, tau, s, use_eig = calibrate_alpha_gamma_s_cv3_conservative(\n",
    "        Xs_tr, Y_tr, B, F, D,\n",
    "        alpha_grid=alpha_grid, Kfolds=Kfolds, val_frac=val_frac,\n",
    "        tail_target=tail_target, cov_target=cov_target,\n",
    "        r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "    F_fin, D_fin = finalize_on_dataset(\n",
    "        Xs_te, Y_te, B, F, D, alpha, gamma, tau, s,\n",
    "        use_eig=use_eig, r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "\n",
    "    Yhat_te = predict_Y(Xs_te, B)\n",
    "    test_mse = mse(Y_te, Yhat_te)\n",
    "    test_nll_cal = test_nll(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "    diag_te = coverage_and_mahalanobis(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "\n",
    "    metrics = {\n",
    "        \"mode\": \"SUR\", \"solver\": \"EM\",\n",
    "        \"K\": K, \"p\": p, \"k\": k,\n",
    "        \"N_tr\": N_tr, \"N_te\": N_te,\n",
    "        \"lam_F\": lam_F, \"lam_B\": lam_B,\n",
    "        \"alpha\": alpha, \"gamma\": gamma, \"tau\": tau, \"s\": s,\n",
    "        \"Sec_fit\": sec_fit, \"Mem_MB_est\": mem_mb,\n",
    "        \"Test_MSE\": test_mse, \"Test_NLL_perN_cal\": test_nll_cal,\n",
    "    }\n",
    "    return metrics, (B, F, D), (F_fin, D_fin), diag_tr, diag_te\n",
    "\n",
    "def run_gls_pipeline_als_cv31(*,\n",
    "    N_tr, N_te, p_list, k, seed,\n",
    "    lam_F=1e-3, lam_B=1e-3, sweeps=12,\n",
    "    alpha_grid=np.linspace(0.78, 1.10, 17), Kfolds=3, val_frac=0.5,\n",
    "    tail_target=0.03, cov_target=0.975, r_boost=2, d_floor=1e-6,\n",
    "    use_cg_beta=True, cg_maxit=800, cg_tol=3e-7, use_diag_precond=True\n",
    "):\n",
    "    K = len(p_list)\n",
    "    Xs_tr, Y_tr, Xs_te, Y_te = simulate_gls(N_tr, N_te, p_list, k, seed=seed)\n",
    "\n",
    "    t0 = time.time()\n",
    "    B, F, D, mem_mb, _ = als_gls(\n",
    "        Xs_tr, Y_tr, k,\n",
    "        lam_F=lam_F, lam_B=lam_B, sweeps=sweeps, d_floor=d_floor,\n",
    "        use_cg_beta=use_cg_beta, cg_maxit=cg_maxit, cg_tol=cg_tol,\n",
    "        use_diag_precond=use_diag_precond\n",
    "    )\n",
    "    sec_fit = time.time() - t0\n",
    "\n",
    "    diag_tr = coverage_and_mahalanobis(Y_tr, Xs_tr, B, F, D)\n",
    "\n",
    "    alpha, gamma, tau, s, use_eig = calibrate_alpha_gamma_s_cv3_conservative(\n",
    "        Xs_tr, Y_tr, B, F, D,\n",
    "        alpha_grid=alpha_grid, Kfolds=Kfolds, val_frac=val_frac,\n",
    "        tail_target=tail_target, cov_target=cov_target,\n",
    "        r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "    F_fin, D_fin = finalize_on_dataset(\n",
    "        Xs_te, Y_te, B, F, D, alpha, gamma, tau, s,\n",
    "        use_eig=use_eig, r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "\n",
    "    Yhat_te = predict_Y(Xs_te, B)\n",
    "    test_mse = mse(Y_te, Yhat_te)\n",
    "    test_nll_cal = test_nll(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "    diag_te = coverage_and_mahalanobis(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "\n",
    "    metrics = {\n",
    "        \"mode\": \"GLS\", \"solver\": \"ALS\",\n",
    "        \"K\": K, \"p_list_summary\": (min(p_list), int(np.median(p_list)), max(p_list)),\n",
    "        \"k\": k, \"N_tr\": N_tr, \"N_te\": N_te,\n",
    "        \"lam_F\": lam_F, \"lam_B\": lam_B,\n",
    "        \"alpha\": alpha, \"gamma\": gamma, \"tau\": tau, \"s\": s,\n",
    "        \"Sec_fit\": sec_fit, \"Mem_MB_est\": mem_mb,\n",
    "        \"Test_MSE\": test_mse, \"Test_NLL_perN_cal\": test_nll_cal,\n",
    "    }\n",
    "    return metrics, (B, F, D), (F_fin, D_fin), diag_tr, diag_te\n",
    "\n",
    "def run_gls_pipeline_em_cv31(*,\n",
    "    N_tr, N_te, p_list, k, seed,\n",
    "    lam_F=1e-3, lam_B=1e-3, iters=45,\n",
    "    alpha_grid=np.linspace(0.78, 1.10, 17), Kfolds=3, val_frac=0.5,\n",
    "    tail_target=0.03, cov_target=0.975, r_boost=2, d_floor=1e-6\n",
    "):\n",
    "    K = len(p_list)\n",
    "    Xs_tr, Y_tr, Xs_te, Y_te = simulate_gls(N_tr, N_te, p_list, k, seed=seed)\n",
    "\n",
    "    t0 = time.time()\n",
    "    B, F, D, mem_mb, _ = em_gls(\n",
    "        Xs_tr, Y_tr, k, lam_F=lam_F, lam_B=lam_B,\n",
    "        iters=iters, d_floor=d_floor\n",
    "    )\n",
    "    sec_fit = time.time() - t0\n",
    "\n",
    "    diag_tr = coverage_and_mahalanobis(Y_tr, Xs_tr, B, F, D)\n",
    "\n",
    "    alpha, gamma, tau, s, use_eig = calibrate_alpha_gamma_s_cv3_conservative(\n",
    "        Xs_tr, Y_tr, B, F, D,\n",
    "        alpha_grid=alpha_grid, Kfolds=Kfolds, val_frac=val_frac,\n",
    "        tail_target=tail_target, cov_target=cov_target,\n",
    "        r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "    F_fin, D_fin = finalize_on_dataset(\n",
    "        Xs_te, Y_te, B, F, D, alpha, gamma, tau, s,\n",
    "        use_eig=use_eig, r_boost=r_boost, d_floor=d_floor\n",
    "    )\n",
    "\n",
    "    Yhat_te = predict_Y(Xs_te, B)\n",
    "    test_mse = mse(Y_te, Yhat_te)\n",
    "    test_nll_cal = test_nll(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "    diag_te = coverage_and_mahalanobis(Y_te, Xs_te, B, F_fin, D_fin)\n",
    "\n",
    "    metrics = {\n",
    "        \"mode\": \"GLS\", \"solver\": \"EM\",\n",
    "        \"K\": K, \"p_list_summary\": (min(p_list), int(np.median(p_list)), max(p_list)),\n",
    "        \"k\": k, \"N_tr\": N_tr, \"N_te\": N_te,\n",
    "        \"lam_F\": lam_F, \"lam_B\": lam_B,\n",
    "        \"alpha\": alpha, \"gamma\": gamma, \"tau\": tau, \"s\": s,\n",
    "        \"Sec_fit\": sec_fit, \"Mem_MB_est\": mem_mb,\n",
    "        \"Test_MSE\": test_mse, \"Test_NLL_perN_cal\": test_nll_cal,\n",
    "    }\n",
    "    return metrics, (B, F, D), (F_fin, D_fin), diag_tr, diag_te\n",
    "\n",
    "# =========================\n",
    "# Kwarg splitter (avoid passing ALS-only args to EM)\n",
    "# =========================\n",
    "\n",
    "def _split_kwargs_for_als_em(kwargs):\n",
    "    als_kwargs = dict(kwargs)\n",
    "    em_kwargs  = dict(kwargs)\n",
    "    for k in (\"sweeps\", \"use_cg_beta\", \"cg_maxit\", \"cg_tol\", \"use_diag_precond\"):\n",
    "        em_kwargs.pop(k, None)\n",
    "    return als_kwargs, em_kwargs\n",
    "\n",
    "# =========================\n",
    "# Comparison entry points\n",
    "# =========================\n",
    "\n",
    "def compare_sur_als_vs_em(**kwargs):\n",
    "    print(\"=== SUR: ALS (matrix-free) vs EM (dense) ===\")\n",
    "    als_kw, em_kw = _split_kwargs_for_als_em(kwargs)\n",
    "\n",
    "    als_metrics, _, _, als_tr, als_te = run_sur_pipeline_als_cv31(**als_kw)\n",
    "    em_metrics,  _, _,  em_tr,  em_te = run_sur_pipeline_em_cv31(**em_kw)\n",
    "\n",
    "    print_metrics_block(\"\\n--- ALS SUR ---\", als_metrics, als_tr, als_te)\n",
    "    print_metrics_block(\"\\n--- EM  SUR ---\", em_metrics,  em_tr,  em_te)\n",
    "\n",
    "    print(\"\\n--- SUR: Deltas (ALS - EM) ---\")\n",
    "    for k in [\"Test_MSE\",\"Test_NLL_perN_cal\",\"Sec_fit\",\"Mem_MB_est\"]:\n",
    "        print(f\"{k}: {als_metrics[k] - em_metrics[k]:+.4f}\")\n",
    "\n",
    "def compare_gls_als_vs_em(**kwargs):\n",
    "    print(\"=== GLS: ALS (matrix-free) vs EM (dense) ===\")\n",
    "    als_kw, em_kw = _split_kwargs_for_als_em(kwargs)\n",
    "\n",
    "    als_metrics, _, _, als_tr, als_te = run_gls_pipeline_als_cv31(**als_kw)\n",
    "    em_metrics,  _, _,  em_tr,  em_te = run_gls_pipeline_em_cv31(**em_kw)\n",
    "\n",
    "    print_metrics_block(\"\\n--- ALS GLS ---\", als_metrics, als_tr, als_te)\n",
    "    print_metrics_block(\"\\n--- EM  GLS ---\", em_metrics,  em_tr,  em_te)\n",
    "\n",
    "    print(\"\\n--- GLS: Deltas (ALS - EM) ---\")\n",
    "    for k in [\"Test_MSE\",\"Test_NLL_perN_cal\",\"Sec_fit\",\"Mem_MB_est\"]:\n",
    "        print(f\"{k}: {als_metrics[k] - em_metrics[k]:+.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Script entry\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # SUR comparison (same settings for both solvers)\n",
    "    compare_sur_als_vs_em(\n",
    "        N_tr=240, N_te=120, K=120, p=3, k=8, seed=12345,\n",
    "        lam_F=1e-3, lam_B=1e-3,\n",
    "        sweeps=12,                    # ALS-only\n",
    "        use_cg_beta=True,            # ALS-only\n",
    "        cg_maxit=800, cg_tol=3e-7,   # ALS-only\n",
    "        use_diag_precond=True,       # ALS-only\n",
    "        alpha_grid=np.linspace(0.78, 1.10, 17),\n",
    "        Kfolds=3, val_frac=0.5,\n",
    "        tail_target=0.03, cov_target=0.975,\n",
    "        r_boost=3, d_floor=1e-6\n",
    "    )\n",
    "\n",
    "    # GLS comparison\n",
    "    compare_gls_als_vs_em(\n",
    "        N_tr=240, N_te=120, p_list=[2,6,4,8,3]*24, k=8, seed=2027,\n",
    "        lam_F=1e-3, lam_B=1e-3,\n",
    "        sweeps=12,                    # ALS-only\n",
    "        use_cg_beta=True,            # ALS-only\n",
    "        cg_maxit=800, cg_tol=3e-7,   # ALS-only\n",
    "        use_diag_precond=True,       # ALS-only\n",
    "        alpha_grid=np.linspace(0.78, 1.10, 17),\n",
    "        Kfolds=3, val_frac=0.5,\n",
    "        tail_target=0.03, cov_target=0.975,\n",
    "        r_boost=3, d_floor=1e-6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d9b6c-e58d-40c1-97f2-3f708f4c33e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
